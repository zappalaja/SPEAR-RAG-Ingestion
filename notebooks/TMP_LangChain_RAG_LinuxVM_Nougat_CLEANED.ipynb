{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815cb1d3",
   "metadata": {},
   "source": [
    "# PDF ➜ Nougat Markdown ➜ Chroma (LangChain) RAG — cleaned + annotated\n",
    "\n",
    "This notebook has **two environment sections**:\n",
    "\n",
    "1) **`nougat` env**: run Nougat on PDFs, detect skipped pages, and build **merged Markdown** outputs.\n",
    "2) **`rag` env**: ingest the merged Markdown into a local **Chroma** vector DB using **LangChain + sentence-transformers**.\n",
    "\n",
    "Notes:\n",
    "- Run cells **top-to-bottom**.\n",
    "- If you re-run ingestion, you can either **clear** the Chroma directory or **upsert** with stable IDs (this notebook uses stable IDs).\n",
    "- Paths below assume your Linux VM home directory; adjust `PDF_DIR`, `NOUGAT_OUT`, `MERGED_OUT`, and `CHROMA_DIR` as needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f1bf2-d565-49b8-82af-9db16cdee5f7",
   "metadata": {},
   "source": [
    "## Environment 1: `nougat`\n",
    "\n",
    "Activate your Nougat environment (example):\n",
    "\n",
    "```bash\n",
    "conda activate nougat\n",
    "```\n",
    "\n",
    "Then run the cells in this section to:\n",
    "- discover PDFs\n",
    "- run Nougat\n",
    "- capture and parse skipped-page logs\n",
    "- generate merged Markdown outputs (Nougat text + fallback text for skipped pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc5217b-5f80-4325-9909-d6591a049eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDFs found: 2\n",
      " - J Adv Model Earth Syst - 2020 - Delworth - SPEAR The Next Generation GFDL Modeling System for Seasonal to Multidecadal-1.pdf\n",
      " - J Adv Model Earth Syst - 2020 - Lu - GFDL s SPEAR Seasonal Prediction System Initialization and Ocean Tendency Adjustment -1.pdf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PDF_DIR = Path(\"/home/zappalaj/pdfs\")\n",
    "NOUGAT_OUT = Path(\"/home/zappalaj/nougat_md\")          # nougat raw outputs\n",
    "MERGED_OUT = Path(\"/home/zappalaj/nougat_merged_md\")   # final merged outputs\n",
    "\n",
    "NOUGAT_OUT.mkdir(parents=True, exist_ok=True)\n",
    "MERGED_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdfs = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
    "print(\"PDFs found:\", len(pdfs))\n",
    "for p in pdfs[:5]:\n",
    "    print(\" -\", p.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3aae921-d30d-428b-85b4-d5d4caba2017",
   "metadata": {},
   "source": [
    "Run Nougat and capture logs (including skipped pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf48efcd-d234-4fd0-801b-50cb8f96767b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Nougat: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [10:08<00:00, 304.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nougat runs: ok=2, failed=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess, json, re\n",
    "from tqdm import tqdm\n",
    "\n",
    "def run_nougat(pdf_path: Path, out_dir: Path) -> dict:\n",
    "    cmd = [\"nougat\", str(pdf_path), \"-o\", str(out_dir)]\n",
    "    p = subprocess.run(cmd, text=True, capture_output=True)\n",
    "    return {\n",
    "        \"pdf\": str(pdf_path),\n",
    "        \"returncode\": p.returncode,\n",
    "        \"stdout\": p.stdout,\n",
    "        \"stderr\": p.stderr,\n",
    "        \"cmd\": cmd,\n",
    "    }\n",
    "\n",
    "runs = []\n",
    "for pdf in tqdm(pdfs, desc=\"Running Nougat\"):\n",
    "    r = run_nougat(pdf, NOUGAT_OUT)\n",
    "    runs.append(r)\n",
    "\n",
    "# quick summary\n",
    "ok = sum(r[\"returncode\"] == 0 for r in runs)\n",
    "bad = len(runs) - ok\n",
    "print(f\"Nougat runs: ok={ok}, failed={bad}\")\n",
    "if bad:\n",
    "    for r in runs:\n",
    "        if r[\"returncode\"] != 0:\n",
    "            print(\"FAILED:\", Path(r[\"pdf\"]).name)\n",
    "            print(r[\"stderr\"][-1000:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e8d3b1-a6a2-4de6-975a-7a718035e47f",
   "metadata": {},
   "source": [
    "Parse skipped pages from Nougat logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1946ca2b-075a-4022-a898-b440523aa667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J Adv Model Earth Syst - 2020 - Delworth - SPEAR The Next Generation GFDL Modeling System for Seasonal to Multidecadal-1.pdf skipped pages: [33]\n",
      "J Adv Model Earth Syst - 2020 - Lu - GFDL s SPEAR Seasonal Prediction System Initialization and Ocean Tendency Adjustment -1.pdf skipped pages: [15]\n"
     ]
    }
   ],
   "source": [
    "skip_re = re.compile(r\"Skipping page\\s+(\\d+)\\s+due to repetitions\", re.IGNORECASE)\n",
    "\n",
    "skipped_pages = {}\n",
    "for r in runs:\n",
    "    pdf = Path(r[\"pdf\"])\n",
    "    pages = [int(m.group(1)) for m in skip_re.finditer(r[\"stderr\"])]\n",
    "    skipped_pages[pdf] = sorted(set(pages))\n",
    "\n",
    "# show results\n",
    "for pdf, pages in skipped_pages.items():\n",
    "    if pages:\n",
    "        print(pdf.name, \"skipped pages:\", pages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5709654-bef1-4c8d-a5f4-e1e101177bee",
   "metadata": {},
   "source": [
    "Extract text for specific pages only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e80c776e-91cd-420f-a8ed-1dc7cd3b8ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_pages_text(pdf_path: Path, page_numbers_1based: list[int]) -> dict[int, str]:\n",
    "    \"\"\"\n",
    "    Returns {page_number_1based: extracted_text}\n",
    "    \"\"\"\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    out = {}\n",
    "    for p1 in page_numbers_1based:\n",
    "        idx = p1 - 1  # convert to 0-based\n",
    "        if 0 <= idx < len(reader.pages):\n",
    "            text = reader.pages[idx].extract_text() or \"\"\n",
    "            out[p1] = text.strip()\n",
    "        else:\n",
    "            out[p1] = \"\"\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ab4881-e5ee-4fa4-a52c-7646515623bd",
   "metadata": {},
   "source": [
    "Merge and write outputs (ensuring JSON files were made correctly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bdc90d6-ebbb-48c8-b5d8-9190e385c421",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zappalaj/miniconda3/envs/nougat/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Merging outputs: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 18.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: 2 merged files into /home/zappalaj/nougat_merged_md\n",
      "- J Adv Model Earth Syst - 2020 - Delworth - SPEAR The Next Generation GFDL Modeling System for Seasonal to Multidecadal-1.pdf skipped: [33] -> J Adv Model Earth Syst - 2020 - Delworth - SPEAR The Next Generation GFDL Modeling System for Seasonal to Multidecadal-1.md\n",
      "- J Adv Model Earth Syst - 2020 - Lu - GFDL s SPEAR Seasonal Prediction System Initialization and Ocean Tendency Adjustment -1.pdf skipped: [15] -> J Adv Model Earth Syst - 2020 - Lu - GFDL s SPEAR Seasonal Prediction System Initialization and Ocean Tendency Adjustment -1.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime, json\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Assumes these already exist from earlier cells:\n",
    "# - pdfs: list[Path]\n",
    "# - NOUGAT_OUT: Path\n",
    "# - MERGED_OUT: Path\n",
    "# - skipped_pages: dict[Path, list[int]]\n",
    "# - extract_pages_text(pdf_path, skipped_pages_list) -> dict[int, str]\n",
    "\n",
    "def find_nougat_output_for_pdf(pdf_path: Path, nougat_out_dir: Path) -> Path | None:\n",
    "    \"\"\"\n",
    "    Returns the nougat output corresponding to this pdf, based ONLY on the pdf stem.\n",
    "    This prevents cross-wiring (Delworth pdf pointing to Lu mmd, etc.).\n",
    "    \"\"\"\n",
    "    # Most common nougat outputs\n",
    "    for ext in (\".mmd\", \".md\", \".txt\"):\n",
    "        p = nougat_out_dir / f\"{pdf_path.stem}{ext}\"\n",
    "        if p.exists():\n",
    "            return p\n",
    "\n",
    "    # If nougat writes into subfolders, search but ONLY accept matches containing the pdf stem.\n",
    "    stem = pdf_path.stem.lower()\n",
    "    candidates = []\n",
    "    for ext in (\".mmd\", \".md\", \".txt\"):\n",
    "        candidates.extend(nougat_out_dir.rglob(f\"*{ext}\"))\n",
    "\n",
    "    matches = [p for p in candidates if stem in p.stem.lower()]\n",
    "    if not matches:\n",
    "        return None\n",
    "\n",
    "    # choose newest among the matches\n",
    "    return max(matches, key=lambda p: p.stat().st_mtime)\n",
    "\n",
    "\n",
    "def merge_with_fallback(pdf_path: Path, nougat_md_path: Path | None, skipped: list[int]) -> tuple[str, dict]:\n",
    "    # load nougat text\n",
    "    if nougat_md_path and nougat_md_path.exists():\n",
    "        nougat_text = nougat_md_path.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "    else:\n",
    "        nougat_text = \"\"\n",
    "\n",
    "    fallback = extract_pages_text(pdf_path, skipped) if skipped else {}\n",
    "\n",
    "    parts = []\n",
    "    if nougat_text:\n",
    "        parts.append(nougat_text)\n",
    "    else:\n",
    "        parts.append(f\"# {pdf_path.name}\\n\\n*(No Nougat output found; using fallback only.)*\\n\")\n",
    "\n",
    "    if skipped:\n",
    "        parts.append(\"\\n\\n---\\n\\n## Fallback extracted text for skipped pages\\n\")\n",
    "        for p1 in skipped:\n",
    "            txt = fallback.get(p1, \"\")\n",
    "            if not txt:\n",
    "                txt = \"(No text could be extracted from this page via fallback.)\"\n",
    "            parts.append(f\"\\n\\n### Page {p1}\\n\\n{txt}\\n\")\n",
    "\n",
    "    merged_text = \"\\n\".join(parts)\n",
    "\n",
    "    meta = {\n",
    "        \"pdf\": str(pdf_path),\n",
    "        \"nougat_output\": str(nougat_md_path) if nougat_md_path else None,\n",
    "        \"skipped_pages\": skipped,\n",
    "        \"created_utc\": datetime.datetime.utcnow().isoformat() + \"Z\",\n",
    "        \"note\": \"Nougat markdown plus pypdf fallback for pages Nougat skipped due to repetitions.\",\n",
    "    }\n",
    "    return merged_text, meta\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Merge + write outputs with a sanity check\n",
    "# ------------------------------------------------------------\n",
    "MERGED_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "written = []\n",
    "bad = []\n",
    "\n",
    "for pdf in tqdm(pdfs, desc=\"Merging outputs\"):\n",
    "    nougat_md = find_nougat_output_for_pdf(pdf, NOUGAT_OUT)\n",
    "    skipped = skipped_pages.get(pdf, [])\n",
    "    merged_text, meta = merge_with_fallback(pdf, nougat_md, skipped)\n",
    "\n",
    "    # sanity check: if we found a nougat output, it should correspond to this pdf\n",
    "    if nougat_md is not None and pdf.stem.lower() not in nougat_md.stem.lower():\n",
    "        bad.append((pdf.name, str(nougat_md)))\n",
    "        # hard fail would be: raise RuntimeError(...)\n",
    "        # but we’ll just record it and continue\n",
    "\n",
    "    out_md = MERGED_OUT / (pdf.stem + \".md\")\n",
    "    out_json = MERGED_OUT / (pdf.stem + \".json\")\n",
    "\n",
    "    out_md.write_text(merged_text, encoding=\"utf-8\")\n",
    "    out_json.write_text(json.dumps(meta, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    written.append((pdf.name, out_md, out_json, skipped))\n",
    "\n",
    "print(\"Wrote:\", len(written), \"merged files into\", MERGED_OUT)\n",
    "for name, mdp, jsp, skipped in written[:5]:\n",
    "    print(\"-\", name, \"skipped:\", skipped, \"->\", mdp.name)\n",
    "\n",
    "if bad:\n",
    "    print(\"\\nWARNING: Some nougat outputs did not match the PDF stem (possible naming mismatch):\")\n",
    "    for pdf_name, nougat_path in bad[:10]:\n",
    "        print(\" -\", pdf_name, \"->\", nougat_path)\n",
    "    if len(bad) > 10:\n",
    "        print(\" (and\", len(bad) - 10, \"more)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b206da-9e35-4c02-bc31-e798e11fcec6",
   "metadata": {},
   "source": [
    "Quick sanity check (look at one merged file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b2c7cd1-8a98-4478-83e1-11a62a40b4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample merged file: /home/zappalaj/nougat_merged_md/J Adv Model Earth Syst - 2020 - Delworth - SPEAR The Next Generation GFDL Modeling System for Seasonal to Multidecadal-1.md\n",
      "ther Review , 147(9), 3409–3428. https://doi.org/10.1175/MWR ‐D‐18‐0227.1\n",
      "Collins, M., Knutti, R., Arblaster, J., Dufresne, J. ‐L., Fichefet, T., Gao, X., et al. (2013). Long ‐term climate change: Projections, commit-\n",
      "ments and irreversibility. In T. F. Stocker et al. (Eds.), Climate Change 2013: The Physical Science Basis. Contribution of Working Group I\n",
      "to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change (pp. 1029–1136). Cambridge: Cambridge University\n",
      "Press.\n",
      "Dee, D. P., Uppala, S. M., Simmons, A. J., Berrisford, P., Poli, P., Kobayashi, S., et al. (2011). The ERA‐Interim reanalysis: Conﬁguration and\n",
      "performance of the data assimilation system. Quarterly Journal of the Royal Meteorological Society , 137(656), 553–597. https://doi.org/\n",
      "10.1002/qj.828\n",
      "Delworth, T. L., Broccoli, A. J., Rosati, A., Stouffer, R. J., Balaji, V., Beesley, J. A., et al. (2006). GFDL's CM2 global coupled climate models.\n",
      "Part I: Formulation and simulation characteristics. Journal of Climate, 19(5), 643–674. Retrieved from. http://journals.ametsoc.org/doi/\n",
      "abs/10.1175/JCLI3629.1\n",
      "10.1029/2019MS001895Journal of Advances in Modeling Earth Systems\n",
      "DELWORTH ET AL. 33 of 36\n",
      "Acknowledgments\n",
      "We thank Drs. Leo Donner and John\n",
      "Dunne, as well as two anonymous\n",
      "reviewers, for providing valuable\n",
      "comments on preliminary versions of\n",
      "this manuscript. Financial support for\n",
      "this work was provided through base\n",
      "funding from the National Oceanic and\n",
      "Atmospheric Administration to the\n",
      "Geophysical Fluid Dynamics\n",
      "Laboratory. Model output relevant to\n",
      "this paper can be found online (ftp://\n",
      "data1.gfdl.noaa.gov/users/Tom.\n",
      "Delworth/SPEAR_Documentation_\n",
      "paper/SPEAR).\n",
      " 19422466, 2020, 3, Downloaded from https://agupubs.onlinelibrary.wiley.com/doi/10.1029/2019MS001895, Wiley Online Library on [12/01/2026]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = written[0][1]\n",
    "print(\"Sample merged file:\", sample)\n",
    "text = sample.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "print(text[-2000:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f604bc0-874d-4c61-9219-7b918491dbc7",
   "metadata": {},
   "source": [
    "Create Doc-level hashes (Doc-level hash (e.g., SHA256 of the PDF bytes) stored as metadata: source_hash = sha256(pdf_bytes)) per chunk to prevent uploading duplicate documents to the Chroma Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "417dc24c-a1d6-4969-9492-187ce888f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "\n",
    "def file_sha256(path: Path, chunk_bytes: int = 1024 * 1024) -> str:\n",
    "    \"\"\"Fast-ish SHA256 of a file on disk.\"\"\"\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for block in iter(lambda: f.read(chunk_bytes), b\"\"):\n",
    "            h.update(block)\n",
    "    return h.hexdigest()\n",
    "\n",
    "# Build a stable hash per *PDF* (useful for de-dup + stable IDs later)\n",
    "pdf_hashes = {pdf.name: file_sha256(pdf) for pdf in pdfs}\n",
    "\n",
    "out_path = MERGED_OUT / \"pdf_sha256.json\"\n",
    "out_path.write_text(json.dumps(pdf_hashes, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"✅ Wrote {len(pdf_hashes)} PDF hashes -> {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a221ed-055a-435b-ae9e-d26575ba9fb2",
   "metadata": {},
   "source": [
    "### Ingest merged Markdown into Chroma\n",
    "\n",
    "The next cell is the **single source of truth** for ingestion (no duplicate helper cells elsewhere).\n",
    "Edit the configuration block at the top (`MERGED_MD_DIR`, `CHROMA_DIR`, model name, chunk sizes) and run it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd08c255-5531-4c60-9231-abdc5b1c47e4",
   "metadata": {},
   "source": [
    "## Environment 2: `rag` (LangChain + Chroma ingestion)\n",
    "\n",
    "Activate your RAG environment (example):\n",
    "\n",
    "```bash\n",
    "conda activate rag\n",
    "```\n",
    "\n",
    "This section will:\n",
    "- load merged Markdown from `MERGED_OUT`\n",
    "- sanitize/flatten metadata so Chroma won’t error\n",
    "- chunk documents and embed them with `sentence-transformers`\n",
    "- build (or update) a local Chroma DB on disk\n",
    "\n",
    "If you run into dependency conflicts between Nougat and `sentence-transformers`, that’s expected — keep them in separate envs as you’ve been doing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add9cc4b-92f7-45f4-9c5d-84a3286d6fc7",
   "metadata": {},
   "source": [
    "The cell below:\n",
    "Fixes the “metadata value got [33] which is a list” class of errors permanently.\n",
    "\n",
    "Lets you drop boilerplate (“fallback extracted text…”) that was polluting your top results.\n",
    "\n",
    "Gives you stable IDs so reruns “upsert” instead of duplicating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78899952-5496-43ed-a7b1-354a5671a502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zappalaj/miniconda3/envs/rag_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_18521/2725413480.py:157: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HuggingFaceEmbeddings OK: sentence-transformers/all-MiniLM-L6-v2\n",
      "✅ Loaded 2 markdown docs from /home/zappalaj/nougat_merged_md\n",
      "✅ Chunked into 269 total chunks.\n",
      "✅ Added/updated 269 chunks into collection='nougat_merged'\n",
      "   Persist dir: /home/zappalaj/chroma_db\n",
      "\n",
      "=== Sanity query results ===\n",
      "\n",
      "[1] source=J Adv Model Earth Syst - 2020 - Delworth - SPEAR The Next Generation GFDL Modeling System for Seasonal to Multidecadal-1.md  title=  page=33\n",
      "### Page 33 ...\n",
      "\n",
      "[2] source=J Adv Model Earth Syst - 2020 - Lu - GFDL s SPEAR Seasonal Prediction System Initialization and Ocean Tendency Adjustment -1.md  title=  page=15\n",
      "### Page 15 ...\n",
      "\n",
      "[3] source=J Adv Model Earth Syst - 2020 - Delworth - SPEAR The Next Generation GFDL Modeling System for Seasonal to Multidecadal-1.md  title=  page=None\n",
      "Figure 19: Time series of an index of the Atlantic Meridional Overturning Circulation (AMOC) in the North Atlantic, computed using isopycal coordinates (see Figures 12c and 12d). The index is computed each year as the maximum value of the overturning stream function at 26\\({}^{o}\\)N. Units are in Sverdrups (Sv), where 1 \\(\\rm{Sv}\\) = 10\\({}^{6}\\)\\(\\rm{m}^{3}\\)\\(\\rm{s}^{-1}\\). The red line and symb ...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import re\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List, Tuple, Optional\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "# Recommended: disable Chroma telemetry to avoid OpenTelemetry/protobuf dependency issues\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "MERGED_MD_DIR = Path(\"/home/zappalaj/nougat_merged_md\")   # <-- where your merged .md/.json live\n",
    "VECTOR_DB_DIR = Path(\"/home/zappalaj/chroma_db\")         # <-- where you want chroma to persist\n",
    "COLLECTION    = \"nougat_merged\"\n",
    "\n",
    "CHUNK_SIZE    = 1200\n",
    "CHUNK_OVERLAP = 150\n",
    "\n",
    "EMBED_MODEL   = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# Keep fallback chunks, but sanitize boilerplate\n",
    "SANITIZE_NUGAT_FALLBACK_HEADER = True\n",
    "INFER_PAGE_FROM_TEXT = True\n",
    "\n",
    "# -------------------------\n",
    "# Helpers\n",
    "# -------------------------\n",
    "def sha256_text(s: str) -> str:\n",
    "    return hashlib.sha256(s.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
    "\n",
    "def load_md_and_optional_json(md_path: Path) -> Tuple[str, Dict[str, Any]]:\n",
    "    text = md_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "    meta: Dict[str, Any] = {}\n",
    "    json_path = md_path.with_suffix(\".json\")\n",
    "    if json_path.exists():\n",
    "        try:\n",
    "            meta = json.loads(json_path.read_text(encoding=\"utf-8\", errors=\"ignore\"))\n",
    "            if not isinstance(meta, dict):\n",
    "                meta = {\"raw_meta\": meta}\n",
    "        except Exception as e:\n",
    "            meta = {\"json_parse_error\": str(e)}\n",
    "    return text, meta\n",
    "\n",
    "_PAGE_RE = re.compile(r\"###\\s*Page\\s+(\\d+)\", re.IGNORECASE)\n",
    "def infer_page_from_markdown(text: str) -> Optional[int]:\n",
    "    m = _PAGE_RE.search((text or \"\")[:2000])\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "def normalize_page_value(v: Any) -> Optional[int]:\n",
    "    if v is None:\n",
    "        return None\n",
    "    if isinstance(v, int):\n",
    "        return v\n",
    "    if isinstance(v, str) and v.strip().isdigit():\n",
    "        return int(v.strip())\n",
    "    if isinstance(v, list) and len(v) > 0:\n",
    "        first = v[0]\n",
    "        if isinstance(first, int):\n",
    "            return first\n",
    "        if isinstance(first, str) and first.strip().isdigit():\n",
    "            return int(first.strip())\n",
    "    return None\n",
    "\n",
    "# Detect Nougat fallback boilerplate; rewrite/strip it\n",
    "_FALLBACK_PHRASE_RE = re.compile(\n",
    "    r\"##\\s*Fallback extracted text for skipped pages\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "\n",
    "def sanitize_fallback_header(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove/replace Nougat fallback boilerplate even when it appears inline.\n",
    "    Keeps the rest of the content (e.g., the Page headings and text).\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "\n",
    "    # Remove the phrase anywhere (standalone or inline)\n",
    "    cleaned = re.sub(_FALLBACK_PHRASE_RE, \"\", text)\n",
    "\n",
    "    # Also remove any leftover repeated separators caused by removal\n",
    "    cleaned = cleaned.replace(\"---\", \" \")\n",
    "\n",
    "    # Normalize whitespace (important when the phrase was inline)\n",
    "    cleaned = re.sub(r\"[ \\t]+\", \" \", cleaned)\n",
    "    cleaned = re.sub(r\"\\n{3,}\", \"\\n\\n\", cleaned)\n",
    "\n",
    "    return cleaned.strip()\n",
    "\n",
    "\n",
    "def normalize_source_metadata(md_path: Path, meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    out: Dict[str, Any] = {}\n",
    "    out[\"source_path\"] = str(md_path)\n",
    "    out[\"source_name\"] = md_path.name\n",
    "\n",
    "    for key in [\"title\", \"doc_title\", \"filename\", \"paper_title\"]:\n",
    "        if key in meta and isinstance(meta[key], str) and meta[key].strip():\n",
    "            out[\"title\"] = meta[key].strip()\n",
    "            break\n",
    "\n",
    "    for key in [\"page\", \"page_num\", \"page_number\", \"pages\"]:\n",
    "        if key in meta:\n",
    "            p = normalize_page_value(meta[key])\n",
    "            if p is not None:\n",
    "                out[\"page\"] = p\n",
    "            break\n",
    "\n",
    "    for key in [\"doi\", \"year\", \"authors\", \"journal\"]:\n",
    "        if key in meta:\n",
    "            v = meta[key]\n",
    "            if isinstance(v, (str, int, float, bool)) or v is None:\n",
    "                out[key] = v\n",
    "            elif isinstance(v, list):\n",
    "                out[key] = \", \".join(map(str, v))\n",
    "            elif isinstance(v, dict):\n",
    "                out[key] = json.dumps(v, ensure_ascii=False)\n",
    "            else:\n",
    "                out[key] = str(v)\n",
    "\n",
    "    return out\n",
    "\n",
    "def force_metadata_primitives(meta: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    safe: Dict[str, Any] = {}\n",
    "    for k, v in (meta or {}).items():\n",
    "        if v is None or isinstance(v, (str, int, float, bool)):\n",
    "            safe[k] = v\n",
    "        else:\n",
    "            try:\n",
    "                safe[k] = json.dumps(v, ensure_ascii=False)\n",
    "            except Exception:\n",
    "                safe[k] = str(v)\n",
    "    return safe\n",
    "\n",
    "# -------------------------\n",
    "# Embeddings\n",
    "# -------------------------\n",
    "embeddings = None\n",
    "try:\n",
    "    from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=EMBED_MODEL,\n",
    "        model_kwargs={\"device\": \"cpu\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},\n",
    "    )\n",
    "    print(\"✅ HuggingFaceEmbeddings OK:\", EMBED_MODEL)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"⚠️ HuggingFaceEmbeddings failed; using fallback embedder.\")\n",
    "    print(\"   Reason:\", repr(e))\n",
    "\n",
    "    import numpy as np\n",
    "    from langchain_core.embeddings import Embeddings\n",
    "\n",
    "    class HashEmbeddings(Embeddings):\n",
    "        def __init__(self, dim: int = 384):\n",
    "            self.dim = dim\n",
    "\n",
    "        def _vec(self, text: str) -> List[float]:\n",
    "            h = hashlib.sha256(text.encode(\"utf-8\", errors=\"ignore\")).digest()\n",
    "            arr = np.frombuffer(h, dtype=np.uint8).astype(np.float32)\n",
    "            reps = int(np.ceil(self.dim / arr.size))\n",
    "            v = np.tile(arr, reps)[: self.dim]\n",
    "            norm = np.linalg.norm(v) + 1e-12\n",
    "            return (v / norm).tolist()\n",
    "\n",
    "        def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "            return [self._vec(t) for t in texts]\n",
    "\n",
    "        def embed_query(self, text: str) -> List[float]:\n",
    "            return self._vec(text)\n",
    "\n",
    "    embeddings = HashEmbeddings(dim=384)\n",
    "    print(\"✅ Fallback HashEmbeddings OK (dim=384)\")\n",
    "\n",
    "# -------------------------\n",
    "# 1) Load docs\n",
    "# -------------------------\n",
    "md_files = sorted(MERGED_MD_DIR.rglob(\"*.md\"))\n",
    "if not md_files:\n",
    "    raise FileNotFoundError(f\"No .md files found under {MERGED_MD_DIR}\")\n",
    "\n",
    "docs: List[Document] = []\n",
    "for md_path in md_files:\n",
    "    text, meta = load_md_and_optional_json(md_path)\n",
    "    if not text.strip():\n",
    "        continue\n",
    "\n",
    "    base_meta = normalize_source_metadata(md_path, meta)\n",
    "    base_meta = force_metadata_primitives(base_meta)\n",
    "    docs.append(Document(page_content=text, metadata=base_meta))\n",
    "\n",
    "print(f\"✅ Loaded {len(docs)} markdown docs from {MERGED_MD_DIR}\")\n",
    "\n",
    "# -------------------------\n",
    "# 2) Chunk\n",
    "# -------------------------\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "all_chunks: List[Document] = []\n",
    "all_ids: List[str] = []\n",
    "\n",
    "for d in docs:\n",
    "    chunks = splitter.split_documents([d])\n",
    "    for idx, c in enumerate(chunks):\n",
    "        # Sanitize the repeated boilerplate header without dropping the chunk\n",
    "        if SANITIZE_NUGAT_FALLBACK_HEADER:\n",
    "            c.page_content = sanitize_fallback_header(c.page_content)\n",
    "\n",
    "        # Infer page from text if missing\n",
    "        if INFER_PAGE_FROM_TEXT and c.metadata.get(\"page\") is None:\n",
    "            p = infer_page_from_markdown(c.page_content)\n",
    "            if p is not None:\n",
    "                c.metadata[\"page\"] = p\n",
    "\n",
    "        # final metadata safety\n",
    "        c.metadata = force_metadata_primitives(c.metadata)\n",
    "\n",
    "        # stable IDs\n",
    "        src_name = c.metadata.get(\"source_name\", \"unknown\")\n",
    "        page = c.metadata.get(\"page\", \"\")\n",
    "        content_hash = sha256_text(c.page_content)[:16]\n",
    "        cid = f\"{src_name}|p{page}|{idx}|{content_hash}\"\n",
    "\n",
    "        all_ids.append(cid)\n",
    "        all_chunks.append(c)\n",
    "\n",
    "print(f\"✅ Chunked into {len(all_chunks)} total chunks.\")\n",
    "\n",
    "# -------------------------\n",
    "# 3) Chroma-safe metadata\n",
    "# -------------------------\n",
    "all_chunks = filter_complex_metadata(all_chunks)\n",
    "all_chunks = [Document(page_content=c.page_content, metadata=force_metadata_primitives(c.metadata)) for c in all_chunks]\n",
    "\n",
    "# -------------------------\n",
    "# 4) Build / open Chroma + ingest\n",
    "# -------------------------\n",
    "client = chromadb.PersistentClient(\n",
    "    path=str(VECTOR_DB_DIR),\n",
    "    settings=Settings(anonymized_telemetry=False),\n",
    ")\n",
    "\n",
    "vectordb = Chroma(\n",
    "    collection_name=COLLECTION,\n",
    "    persist_directory=str(VECTOR_DB_DIR),\n",
    "    embedding_function=embeddings,\n",
    "    client=client,\n",
    ")\n",
    "\n",
    "vectordb.add_documents(all_chunks, ids=all_ids)\n",
    "\n",
    "print(f\"✅ Added/updated {len(all_chunks)} chunks into collection='{COLLECTION}'\")\n",
    "print(f\"   Persist dir: {VECTOR_DB_DIR.resolve()}\")\n",
    "\n",
    "# -------------------------\n",
    "# 5) Quick sanity query\n",
    "# -------------------------\n",
    "query = \"What is this document about?\"\n",
    "results = vectordb.similarity_search(query, k=3)\n",
    "\n",
    "print(\"\\n=== Sanity query results ===\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    src = r.metadata.get(\"source_name\", r.metadata.get(\"source_path\", \"unknown\"))\n",
    "    title = r.metadata.get(\"title\", \"\")\n",
    "    page = r.metadata.get(\"page\", None)\n",
    "    print(f\"\\n[{i}] source={src}  title={title}  page={page}\")\n",
    "    print(r.page_content[:400].replace(\"\\n\", \" \").strip(), \"...\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
